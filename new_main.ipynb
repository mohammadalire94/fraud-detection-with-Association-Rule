{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from itertools import combinations , product\n",
    "import json\n",
    "from functions import get_kandid , k_candidate_list , add_class_to_tuple , confidence_for_K_candidate_list  ,add_class_to_tuple , k_1_common ,remove_class , labeling , calsiffication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepaire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('D:/AmirKabir/term 4/big data/hw1/BigData-HW1/P2/creditcard.csv')\n",
    "dataset = dataset.drop(['Time'], axis=1)\n",
    "\n",
    "#normalization v1-v28 between -1 and 1 with for loops\n",
    "for i in range(1,29):\n",
    "    dataset['V'+str(i)] = (dataset['V'+str(i)] - dataset['V'+str(i)].min()) / (dataset['V'+str(i)].max() - dataset['V'+str(i)].min()) * 2 - 1\n",
    "\n",
    "#normalization amount between -1 and 1 \n",
    "dataset['Amount'] = (dataset['Amount'] - dataset['Amount'].min()) / (dataset['Amount'].max() - dataset['Amount'].min()) * 2 - 1\n",
    "amount_quartiles = pd.qcut(dataset['Amount'], 20 ,labels=['1', '2', '3', '4', '5', '6','7','8','9', '10', '11', '12', '13', '14', '15','16','17','18', '19', '20'])\n",
    "amount_quartile_dummies = pd.get_dummies(amount_quartiles, prefix='Amount')\n",
    "\n",
    "# Concatenate the original dataset with the new columns\n",
    "dataset = pd.concat([dataset, amount_quartile_dummies], axis=1)\n",
    "dataset = dataset.drop(['Amount'], axis=1)\n",
    "\n",
    "for i in range(1,29):\n",
    "    v_quartiles = pd.qcut(dataset['V'+str(i)], 20 , labels=['1', '2', '3', '4', '5', '6','7','8','9', '10', '11', '12', '13', '14', '15','16','17','18', '19', '20'])\n",
    "    v_quartile_dummies = pd.get_dummies(v_quartiles, prefix='V'+str(i))\n",
    "    dataset = pd.concat([dataset, v_quartile_dummies], axis=1)\n",
    "    dataset = dataset.drop(['V'+str(i)], axis=1)\n",
    "\n",
    "dataset['Class'] = dataset['Class'].replace(0, 'normal')\n",
    "dataset['Class'] = dataset['Class'].replace(1, 'fraud')\n",
    "class_dummies = pd.get_dummies(dataset['Class'], prefix='class')\n",
    "dataset = pd.concat([dataset, class_dummies], axis=1)\n",
    "dataset = dataset.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_kandid = get_kandid(dataset)\n",
    "two_kandid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_frequency = {item: dataset[item].sum() for item in dataset.columns}\n",
    "item_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_columns_list = list(dataset.columns)\n",
    "dataset_columns_list.remove('class_normal')\n",
    "dataset_columns_list.remove('class_fraud')\n",
    "dataset_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_list = k_candidate_list(2,dataset_columns_list)\n",
    "len(candidate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for pair in two_kandid:\n",
    "    sup = len(dataset[dataset[pair[0]] + dataset[pair[1]] == 2])\n",
    "    dic[pair] = sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dictionary with tuple keys\n",
    "my_dict = dic\n",
    "\n",
    "# Convert tuple keys to strings\n",
    "my_dict_str = {str(k): v for k, v in my_dict.items()}\n",
    "\n",
    "# Save dictionary to a JSON file\n",
    "with open('D:/AmirKabir/term 4/big data/hw1/BigData-HW1/P2/dic.json', 'w') as fp:\n",
    "    json.dump(my_dict_str, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dictionary from the JSON file\n",
    "with open('D:/AmirKabir/term 4/big data/hw1/BigData-HW1/P2/dic.json', 'r') as fp:\n",
    "    my_dict_str = json.load(fp)\n",
    "# Convert string keys back to tuples\n",
    "dic = {eval(k): v for k, v in my_dict_str.items()}\n",
    "# Print the dictionary\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sup_two = {key: value for key, value in dic.items() if 500 < value < 800}\n",
    "len(filtered_sup_two) , filtered_sup_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_fraud = add_class_to_tuple(filtered_sup_two)\n",
    "candidate_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can read from support_dic and confidence_dic in the zip file instead of running (for save time)\n",
    "confidence_dic , support_dic = confidence_for_K_candidate_list(candidate_fraud, dataset, filtered_sup = False)\n",
    "my_confidence_dic = confidence_dic\n",
    "my_dict_str = {str(k): v for k, v in my_confidence_dic.items()}\n",
    "with open('D:/AmirKabir/term 4/big data/hw1/BigData-HW1/P2/confidence_dic.json', 'w') as fp:\n",
    "    json.dump(my_dict_str, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dictionary from the JSON file\n",
    "with open('D:/AmirKabir/term 4/big data/hw1/BigData-HW1/P2/support_dic.json', 'r') as fp:\n",
    "    my_dict_str = json.load(fp)\n",
    "support_dic = {eval(k): v for k, v in my_dict_str.items()}\n",
    "support_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dictionary from the JSON file\n",
    "with open('D:/AmirKabir/term 4/big data/hw1/BigData-HW1/P2/confidence_dic.json', 'r') as fp:\n",
    "    my_dict_str = json.load(fp)\n",
    "confidence_dic = {eval(k): v for k, v in my_dict_str.items()}\n",
    "confidence_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate interest \n",
    "class_fraud_prob = len(dataset[dataset['class_fraud'] == 1]) / len(dataset)\n",
    "class_normal_prob = 1 - class_fraud_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_confidence_dic = list(sorted(confidence_dic.values()))\n",
    "list_sup_dic = list(sorted(support_dic.values()))\n",
    "len(list_confidence_dic)\n",
    "list_confidence_dic[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate interest\n",
    "interest_dic_fraud_two = {}\n",
    "for key in confidence_dic.keys():\n",
    "    interest_dic_fraud_two[key] = confidence_dic[key] - class_fraud_prob\n",
    "\n",
    "sorted_interest_dic_fraud_two = sorted(interest_dic_fraud_two.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_interest_dic_fraud_two[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items_for_fraud = {key: value for key, value in confidence_dic.items() if value > 0.5}\n",
    "top_items_for_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_two_items_for_fraud = list(top_items_for_fraud.keys())\n",
    "top_two_items_for_fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class normal three top basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(dic)*0.01)  # calculate 1% of the length of the dictionary\n",
    "sorted_sup_two_normal = dict(sorted(dic.items(), key=lambda item: item[1], reverse=True)[:n])  \n",
    "len(sorted_sup_two_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_noraml = add_class_to_tuple(sorted_sup_two_normal)\n",
    "candidate_noraml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_for_K_candidate_list(K_candidate_list, dataset, filtered_sup = True ):\n",
    "    confidence_dic = {}\n",
    "    support_dic = {}\n",
    "    for pairs in K_candidate_list: \n",
    "        objects = np.array(pairs[:-1]).reshape(1,-1)[0]\n",
    "        clas =  pairs[-1]\n",
    "        class_object_support = len(dataset[dataset[objects].sum(axis=1) + dataset[clas] == len(objects) + 1])\n",
    "        \n",
    "        if filtered_sup == True:\n",
    "            filtered_sup = sorted_sup_two_normal\n",
    "            object_support = filtered_sup[tuple(objects)]\n",
    "        else:\n",
    "            object_support = len(dataset[dataset[objects].sum(axis=1) == len(objects)])\n",
    "\n",
    "        if object_support == 0:\n",
    "            confidence = 0\n",
    "            class_object_support = 0\n",
    "        else:\n",
    "            confidence = class_object_support / object_support\n",
    "\n",
    "        confidence_dic[pairs] =  confidence\n",
    "        support_dic[pairs] = class_object_support\n",
    "    return confidence_dic , support_dic\n",
    "\n",
    "confidence_dic_normal_three , support_dic_normal_three = confidence_for_K_candidate_list(candidate_noraml, dataset , filtered_sup= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_confidenc_dic_normal_three = {key:value for key,value in sorted(confidence_dic_normal_three.items(), key=lambda item: item[1], reverse=True)}\n",
    "list(filt_confidenc_dic_normal_three.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_int = list(filt_confidenc_dic_normal_three.items())[0:5]\n",
    "cal_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_dic_fraud_two = {}\n",
    "for tup in cal_int:\n",
    "    interest_dic_fraud_two[tup[0]] = tup[1] - class_normal_prob\n",
    "interest_dic_fraud_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_basket_3 = {key : value for key, value in confidence_dic_normal_three.items() if value == 1 }\n",
    "basket = list(final_basket_3.keys())\n",
    "len(basket) , basket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## four size basket for class fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_items_for_fraud = {key: value for key, value in confidence_dic.items() if value > 0.2}\n",
    "top_items_for_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items_for_fraud_list = list(top_items_for_fraud.keys())\n",
    "len(top_items_for_fraud_list) , top_items_for_fraud_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_data = []\n",
    "for data in top_items_for_fraud_list:\n",
    "    id = data.index('class_fraud')\n",
    "    data = list(data)\n",
    "    data.pop(id)\n",
    "    data = tuple(data)\n",
    "    four_data.append(data)\n",
    "four_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_candidate = k_1_common(four_data)\n",
    "len(three_candidate) , three_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_candidate_list_fraud = []\n",
    "three_candidate  = list(three_candidate)\n",
    "for i in three_candidate:\n",
    "    i = list(i)\n",
    "    i.append('class_fraud')\n",
    "    i = tuple(i)\n",
    "    four_candidate_list_fraud.append(i)\n",
    "four_candidate_list_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_dic_four_fraud , support_dic_four_fraud = confidence_for_K_candidate_list(four_candidate_list_fraud,\n",
    " dataset , filtered_sup = False )\n",
    "sorted_confidence_dic_four_fraud = dict(sorted(confidence_dic_four_fraud.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_confidence_dic_four_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sorted_confidence_dic_four_fraud.items())[:5]\n",
    "dic_interste_fraud = {}\n",
    "cal_interset_fraud = list(sorted_confidence_dic_four_fraud.items())[:5]\n",
    "for data in cal_interset_fraud:\n",
    "    dic_interste_fraud[data[0]] = data[1] - class_fraud_prob\n",
    "dic_interste_fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## four size basket for class normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_basket_3 = {key : value for key, value in confidence_dic_normal_three.items() if value == 1 }\n",
    "basket = list(final_basket_3.keys())\n",
    "len(basket) ,basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items_for_normal_list  = basket\n",
    "four_data_normal = []\n",
    "for data in top_items_for_normal_list:\n",
    "    id = data.index('class_normal')\n",
    "    data = list(data)\n",
    "    data.pop(id)\n",
    "    data = tuple(data)\n",
    "    four_data_normal.append(data)\n",
    "len(four_data_normal) ,four_data_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_candidate_normal = k_1_common(four_data_normal)\n",
    "three_candidate_normal ,len(three_candidate_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_candidate_list_normal = []\n",
    "three_candidate_normal  = list(three_candidate_normal)\n",
    "for i in three_candidate_normal:\n",
    "    i = list(i)\n",
    "    i.append('class_normal')\n",
    "    i = tuple(i)\n",
    "    four_candidate_list_normal.append(i)\n",
    "four_candidate_list_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_dic_four_normal , support_dic_four_normal = confidence_for_K_candidate_list(four_candidate_list_normal,\n",
    " dataset , filtered_sup= False )\n",
    "sorted_confidence_dic_four_normal = dict(sorted(confidence_dic_four_normal.items(), key=lambda item: item[1], reverse=True))\n",
    "len(sorted_confidence_dic_four_normal) , sorted_confidence_dic_four_normal.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_support_dic_four_normal = dict(sorted(support_dic_four_normal.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_support_dic_four_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sorted_confidence_dic_four_normal = {key : value for key, value in sorted_confidence_dic_four_normal.items() if value ==1}\n",
    "top_sorted_confidence_dic_four_normal = list(top_sorted_confidence_dic_four_normal.keys())\n",
    "len(top_sorted_confidence_dic_four_normal) , top_sorted_confidence_dic_four_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_top_sorted_confidence_dic_four_normal =top_sorted_confidence_dic_four_normal[:5]\n",
    "cal_top_sorted_confidence_dic_four_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_dic_normal = {}\n",
    "for tup in cal_top_sorted_confidence_dic_four_normal:\n",
    "    interest_dic_normal[tup[0]] = tup[1] - class_normal_prob\n",
    "interest_dic_normal  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_sorted_confidence_dic_four_normal = remove_class(top_sorted_confidence_dic_four_normal)\n",
    "top_sorted_confidence_dic_four_fraud = remove_class(list(sorted_confidence_dic_four_fraud.keys())[:5])\n",
    "final_basket_3 = remove_class(list(final_basket_3.keys()))\n",
    "top_two_items_for_fraud = remove_class(top_two_items_for_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all the rules and labels\n",
    "normal_label = 'normal'\n",
    "fraud_label = 'fraud'\n",
    "A = ([tuple(sorted(tup)) for tup in top_sorted_confidence_dic_four_normal] ,  normal_label)\n",
    "B = ([tuple(sorted(tup)) for tup in top_sorted_confidence_dic_four_fraud] , fraud_label)\n",
    "C = ([tuple(sorted(tup)) for tup in final_basket_3] , normal_label)\n",
    "D = ([tuple(sorted(tup)) for tup in top_two_items_for_fraud] , fraud_label)\n",
    "final_list_fraud = [B,D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_list = labeling(dataset)\n",
    "count = 0\n",
    "for i in predicted_label_list:\n",
    "    if i == 1:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy , precision , recall , f1_score , TP ,  FP , TN , FN = calsiffication(predicted_label_list , first_dataset)\n",
    "accuracy , precision , recall , f1_score\n",
    "TP ,  FP , TN , FN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
